---
layout: single
classes: wide
title:  "Understanding Nonlinearity"
date:   2025-05-11
permalink: /learning/understanding-nonlinearity
tags: machine learning, thoughts
---

A while ago, a friend showed me a [paper](https://arxiv.org/abs/2401.17992) introducting multilinear operator networks, or MONets for short. The introduction had a note about how some applications of deep learning architectures require different approaches to model nonlinearity. As a concrete example, they mention that certain encryption schemes for sensitive information (specifically LHFE) only supports addition and multiplication, and in particular do not support the nonlinear activation functions that are the bread and butter of neural networks. The end resut of their paper was a novel architecture that is on part with state-of-the-art on image recognition. 

Something that struck me while reading the paper is that the cutting edge of deep learning is primarily heuristic driven--all too often the reason xyz is used in the model is because someone published a paper or ran an experiment that showed that it was better. Indeed, generally there are no theoretical reasons for any one model architecture (e.g. transformers) to be better than another for a given task, and I wish there existed some mathematical or statistical framework for concrete guarantees on the effectiveness for a given neural network. The unfortunate side effect of this is that as models and data scale up, it will become increasingly difficult and expensive to understand what reasonable expectations for a new framework should be, beyond some 'theoretical guarantee' that typically looks like 

> For any $$\varepsilon > 0$$, there exists $$N \gg 0$$ such that a model with $$N$$ parameters will have loss at most $$\varepsilon$$ greater than the theoretical minimal loss. 

I think that concrete understanding of large ML models will start from understanding how nonlinearity plays into the equation. This blog post will be a collection of my own thoughts and musings on how nonlinearity appears within deep neural networks, and an overview of some ML literature that I've read in this area. If you have questions, comments, or suggestions, please do not hesitate to send me an email or leave a comment below. Apologies in advance for potentially sketchy math.

# High Level Overview
Heuristically, the reason why deep learning works in modelling this data is because there exist highly nonlinear correlations within the data. For example, the training objective for an autoregressive text model (e.g. ChatGPT) is to predict the next token conditional on the last $$N$$ tokens, where usually $$N$$ is a large number--as of 2025, cutting edge language models support anywhere from 32 thousand to 10 million tokens of context. Fortunately for us, human language exhibits patterns on a large range of scales, from grammar at the sentence level, to logical arguments at the paragraph level, etc. so it's reasonable to assume that there are many nonlinear correlations that a model potentially could learn in order to  help it predict the next token.  One only needs to take a look at the [Wikipedia page](https://en.wikipedia.org/wiki/Natural_language_processing#History) for natural language processing to see that it is for these reasons that people have always believed language modelling to be a solvable problem. 

As another example, we can also look in the world of computer vision. Deep convolutional networks and vision transformers are trained to recognize patterns on pixel level data, whether it be for denoising or for image recognition. As with the case of language modelling, this has [historical roots](https://en.wikipedia.org/wiki/Neocognitron) based off of the belief that there are many local-to-global patterns, within image data. Common heuristics that get mentioned when people learn about computer vision are things like edge detection at the pixel level, to object and character detection at a larger but local level, to describing entire images at a more global scale.   

The real breakthough of the last decade has been the tools to build the right kind of large, nonlinear model that can effectively build an internal representation of these highly nonlinear patterns. Unfortunately, getting the right framework to generate the nonlinearity isn't a given--stacking a bunch of layers with random initialization and training with the pytorch autograd can land you in tricky situations such as gradient vanishing or gradient exploding. 

As a mathematical note, the ML model is a function from your data to the labels (e.g. next token, denoised image, ground truth, etc.)
$$
f_{model}: \mathbb{R}^{d_\text{data}} \rightarrow \mathbb{R}^{d_\text{labels}}
$$
whereas the loss landscape is the graph of the loss function $$\ell$$
$$
\ell : \mathbb{R}^{d_\text{model}} \rightarrow \mathbb{R}
$$
So that the model outputs will be always be a continuous, and in fact [Lipschitz](https://en.wikipedia.org/wiki/Lipschitz_continuity) function of the input data. The issue is that even though your model might theoretically be infinitely flexible and capable of modelling any function of the input space, there are no a priori guarantees that your model will have a favorable loss landscape for the problem that you are dealing with. Since the typical paradigm for training deep neural networks is some modification stochastic gradient descent, the gradient of the model parameters will be a random variable depending on the parameters themselves as well as the underlying data distribution. So a poorly designed model will have highly variable and fluctuating gradients between minibatches. Indeed, for any given minibatch, there could be many regions with bad gradient behavior.

As an aside, a quick first test for your model design and initialization is to take a single minibatch and run gradient descent on only that minibatch. If your model converges reasonably quickly to an overfit, then you can be reasonably confident that you don't have issues within you minibatch. On the other hand if your model has performance issues, then it might be time to adjust some of your hyperparameters (e.g. learn rate, model design). 


# Where Does Nonlinearity Live?
As a bit of an overgeneralization, deep neural networks alternate between nonlinear layers and linear layers--without the nonlinearity in between, the composition of two linear layers will simply be another linear layer. Of course, in practice modern architectures are  more complicated than just a stack of layers.

## Activation Functions
The simplest way to have nonlinearity--we simply stick an elementwise activation function between 2 linear layers:
<pre class="mermaid">
flowchart LR
    A["in"] -- Linear --> B("$$\mathbb{R}^{d_{mid}}$$")
    B -- Nonlinear activation --> C("$$\mathbb{R}^{d_{mid}}$$")
    C -- Linear --> D("out")
</pre>
There are a multitude of activation functions that we can choose from. During the early days of ML people preferred things like the sigmoid, but more modern architectures increasingly prefer the rectified linear unit (ReLU) and it's variants.
- Sigmoid: $$\mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}$$ (Note that up to scaling and shifting the input/output this is equivalent to $$\tanh$$)
- ReLU: $$\mathrm{ReLU}(x) = \max(x,0)$$
- Softplus: $$\mathrm{softplus}(x) = \ln(1 + e^x)$$
- GELU: $$\mathrm{GELU}(x) = x\Phi(x)$$, where $$\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{t^2/2} dt$$ is the CDF of the standard normal distribution. 
- SiLU (swish): $$\mathrm{SiLU}_\beta(x) = x  \cdot \mathrm{sigmoid}(\beta x)= \frac{x}{1 + e^{-\beta x}}$$ 

There are also gated linear unit (GLU) variants that use 2 different separate linear layers to create the nonlinearity:
<pre class="mermaid">
flowchart LR
    A["in"] -- linear --> x["x"] & z["z"]
    x --> GLU
    z --> GLU
    GLU --> out
</pre>
Heuristically, this allows the model to 'learn' the activation functions via the linear layer that outputs $$z$$, but it is hard to say whether we get a real performance increase. Indeed, the original GLU [paper](https://arxiv.org/pdf/2002.05202) attributes the success to 'divine benevolence'.
- GLU: $$\mathrm{GLU}_{z}(x) = (z) \cdot \mathrm{sigmoid}(x)$$
- ReGLU: $$\mathrm{ReGLU}_{z}(x) = (z) \cdot \mathrm{ReLU}(x)$$
- GEGLU: $$\mathrm{GEGLU}_{z}(x) = (\z) \cdot \mathrm{GELU}(x)$$
- SwiGLU: $$\mathrm{SwiGLU}_{z, \beta}(x) = (z) \cdot \mathrm{SiLU}_{\beta}(x)$$
Note that one can essentially define a GLU activation function for any given activation function above. 


## Layer Norm
Another way nonlinearity is introduced into a model is via the layernorm operation. Suppose that $x \in \mathbb{R}^d$$ is some vector. Then the entry-wise mean $$\mu$$ and variance $$\sigma$$ can be found by the formulas
$$
\begin{split}
\mu &= \frac{1}{d} \sum x_i\\
\sigma^2 &= \frac{1}{d} \sum (x_i - \mu)^2
$$
Then we can write 
$$ 
\mathrm{LayerNorm}(x)_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \varepsilon}}
$$
Geometrically, what we are doing is first projecting $$x$$ down to the codimension 1 linear subspace $$ \{\sum x_i = 0\}\subset \mathbb{R}^d $$ and then projecting the output of that part down onto the sphere $$ S^{d-2} = \{ \sum x_i^2 = d \} \subset \{\sum x_i = 0\}$$ with a $$\varepsilon$$ regularizer for numerical stability. The former operation is linear, but the latter operation is much more nonlinear. 

As an aside, I wonder if the first linear projection is truly necessary. After all, we are paying for a $$d$$-dimensional residual stream; why are we arbitrarily projecting out one dimension? If anyone has thoughts on the matter, or has a high quality reference with an answer, please leave a comment down below. I'll gladly venmo 5 dollars to the first person to leave a high quality reply. 

## Softmax



## Self-Attention

The $$qk$$ component of self-attention is a second-order polynomial. 


## Polynomial Networks



# Keeping Gradients Consistent

## Residual Connections


## Kaiming He Initialization


## Learning Rate Adjustments



# Quantifying Nonlinearity

## Nonlinearity in a Single Layer
Single layers are bad at modelling extremely nonlinear functions

## Quantization
How much floating point precision do we even need? 
