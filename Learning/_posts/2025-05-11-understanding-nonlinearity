---
layout: single
classes: wide
title:  "Understanding Nonlinearity"
date:   2025-05-11
permalink: /learning/understanding-nonlinearity
---

A while ago, a friend showed me a [paper](https://arxiv.org/abs/2401.17992) introducting multilinear operator networks, or MONets for short. The introduction had a note about how some applications of deep learning architectures require different approaches to model nonlinearity. As a concrete example, they mention that certain encryption schemes for sensitive information (specifically LHFE) only supports addition and multiplication, and in particular do not support the nonlinear activation functions that are the bread and butter of neural networks. This blog post will be a collection of my own thoughts and musings on how nonlinearity appears within deep neural networks, and an overview of some ML literature that I've read in this area. 

If you have questions, comments, or suggestions, please do not hesitate to send me an email or leave a comment below.

# High Level Overview
Heuristically, the reason why deep learning works in modelling this data is because there exist highly nonlinear correlations within the data. For example, the training objective for an autoregressive text model (e.g. ChatGPT) is to predict the next token conditional on the last $$N$$ tokens, where usually $$N$$ is a large number--as of 2025, cutting edge language models support anywhere from 32 thousand to 10 million tokens of context. Fortunately for us, human language exhibits patterns on a large range of scales, from grammar at the sentence level, to logical arguments at the paragraph level, etc. so it's reasonable to assume that there are many nonlinear correlations that a model potentially could learn in order to  help it predict the next token.  One only needs to take a look at the [Wikipedia page](https://en.wikipedia.org/wiki/Natural_language_processing#History) for natural language processing to see that it is for these reasons that people have always believed language modelling to be a solvable problem. The real breakthough of the last decade has been the tools to build the right kind of large, nonlinear model that can effectively model these nonlinear functions.  

As another example, we can also look in the world of computer vision. Deep convolutional networks and vision transformers are trained to recognize patterns on pixel level data, whether it be for denoising or for image recognition. As with the case of language modelling, this has [historical roots](https://en.wikipedia.org/wiki/Neocognitron) based off of the belief that there are many local-to-global patterns, within image data. Common heuristics that get mentioned when people learn about computer vision are things like edge detection at the pixel level, to object and character detection at a larger but local level, to describing entire images at a more global scale. 

Unfortunately, getting the right framework to generate the nonlinearity is hard--stacking a bunch of layers with random initialization and training with the pytorch autograd will often land you in tricky situations such as gradient vanishing or gradient exploding. The issue is that by design, the model outputs will be a uniformly [Lipschitz](https://en.wikipedia.org/wiki/Lipschitz_continuity) function of the input data. 
 

# Where Does Nonlinearity Live?

## Activation Functions


## Layer Norm


## Self-Attention


## Polynomial Networks



# Keeping Gradients Consistent

## Residual Connections


## Kaiming He Initialization


## Learning Rate Adjustments



# Quantifying Nonlinearity

## Nonlinearity in a Single Layer
Single layers are bad at modelling extremely nonlinear functions

## Quantization
How much floating point precision do we even need? 
